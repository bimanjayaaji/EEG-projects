{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed14bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad855458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dc4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainflow.board_shim import BoardShim\n",
    "from brainflow.data_filter import DataFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7b507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbcc007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(intercept, coefs, model_type):\n",
    "    coefficients_string = '%s' % (','.join([str(x) for x in coefs[0]]))\n",
    "    file_content = '''\n",
    "#include \"%s\"\n",
    "// clang-format off\n",
    "const double %s_coefficients[%d] = {%s};\n",
    "double %s_intercept = %lf;\n",
    "// clang-format on\n",
    "''' % (f'{model_type}_model.h', model_type, len(coefs[0]), coefficients_string, model_type, intercept)\n",
    "    file_name = f'{model_type}_model.cpp'\n",
    "    file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), '..', 'generated', file_name)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ef3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(first_class, second_class, blacklisted_channels=None):\n",
    "    # use different windows, its kinda data augmentation\n",
    "    window_sizes = [4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "    overlaps = [0.5, 0.475, 0.45, 0.425, 0.4, 0.375, 0.35] # percentage of window_size\n",
    "    dataset_x = list()\n",
    "    dataset_y = list()\n",
    "    for data_type in (first_class, second_class):\n",
    "        for file in glob.glob(os.path.join('data', data_type, '*', '*.csv')):\n",
    "            logging.info(file)\n",
    "            board_id = os.path.basename(os.path.dirname(file))\n",
    "            try:\n",
    "                board_id = int(board_id)\n",
    "                data = DataFilter.read_file(file)\n",
    "                sampling_rate = BoardShim.get_sampling_rate(board_id)\n",
    "                eeg_channels = get_eeg_channels(board_id, blacklisted_channels)\n",
    "                for num, window_size in enumerate(window_sizes):\n",
    "                    cur_pos = sampling_rate * 10\n",
    "                    while cur_pos + int(window_size * sampling_rate) < data.shape[1]:\n",
    "                        data_in_window = data[:, cur_pos:cur_pos + int(window_size * sampling_rate)]\n",
    "                        data_in_window = np.ascontiguousarray(data_in_window)\n",
    "                        bands = DataFilter.get_avg_band_powers(data_in_window, eeg_channels, sampling_rate, True)\n",
    "                        feature_vector = bands[0]\n",
    "                        feature_vector = feature_vector.astype(float)\n",
    "                        dataset_x.append(feature_vector)\n",
    "                        if data_type == first_class:\n",
    "                            dataset_y.append(0)\n",
    "                        else:\n",
    "                            dataset_y.append(1)\n",
    "                        cur_pos = cur_pos + int(window_size * overlaps[num] * sampling_rate)\n",
    "            except Exception as e:\n",
    "                logging.error(str(e), exc_info=True)\n",
    "\n",
    "    logging.info('1st Class: %d 2nd Class: %d' % (len([x for x in dataset_y if x == 0]), len([x for x in dataset_y if x == 1])))\n",
    "\n",
    "    with open('dataset_x.pickle', 'wb') as f:\n",
    "        pickle.dump(dataset_x, f, protocol=3)\n",
    "    with open('dataset_y.pickle', 'wb') as f:\n",
    "        pickle.dump(dataset_y, f, protocol=3)\n",
    "\n",
    "    return dataset_x, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5bded3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eeg_channels(board_id, blacklisted_channels):\n",
    "    eeg_channels = BoardShim.get_eeg_channels(board_id)\n",
    "    try:\n",
    "        eeg_names = BoardShim.get_eeg_names(board_id)\n",
    "        selected_channels = list()\n",
    "        if blacklisted_channels is None:\n",
    "            blacklisted_channels = set()\n",
    "        for i, channel in enumerate(eeg_names):\n",
    "            if not channel in blacklisted_channels:\n",
    "                selected_channels.append(eeg_channels[i])\n",
    "        eeg_channels = selected_channels\n",
    "    except Exception as e:\n",
    "        logging.warn(str(e))\n",
    "    logging.info('channels to use: %s' % str(eeg_channels))\n",
    "    return eeg_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e2b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(data):\n",
    "    x, y = data\n",
    "    first_class_ids = [idx[0] for idx in enumerate(y) if idx[1] == 0]\n",
    "    second_class_ids = [idx[0] for idx in enumerate(y) if idx[1] == 1]\n",
    "    x_first_class = list()\n",
    "    x_second_class = list()\n",
    "    \n",
    "    for i, x_data in enumerate(x):\n",
    "        if i in first_class_ids:\n",
    "            x_first_class.append(x_data.tolist())\n",
    "        elif i in second_class_ids:\n",
    "            x_second_class.append(x_data.tolist())\n",
    "    second_class_np = np.array(x_second_class)\n",
    "    first_class_np = np.array(x_first_class)\n",
    "\n",
    "    logging.info('1st Class Dataset Info:')\n",
    "    logging.info('Mean:')\n",
    "    logging.info(np.mean(first_class_np, axis=0))\n",
    "    logging.info('2nd Class Dataset Info:')\n",
    "    logging.info('Mean:')\n",
    "    logging.info(np.mean(second_class_np, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb07748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_mindfulness(data):\n",
    "    model = SVC(kernel='linear', verbose=True, random_state=1, class_weight='balanced', probability=True)\n",
    "    logging.info('#### SVM ####')\n",
    "    model.fit(data[0], data[1])\n",
    "    initial_type = [('mindfulness_input', FloatTensorType([1, 5]))]\n",
    "    onx = convert_sklearn(model, initial_types=initial_type, target_opset=11, options={type(model): {'zipmap': False}})\n",
    "    with open('svm_mindfulness.onnx', 'wb') as f:\n",
    "        f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0b44fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument('--reuse-dataset', action='store_true')\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    #if args.reuse_dataset:\n",
    "    #    with open('dataset_x.pickle', 'rb') as f:\n",
    "    #        dataset_x = pickle.load(f)\n",
    "    #    with open('dataset_y.pickle', 'rb') as f:\n",
    "    #        dataset_y = pickle.load(f)\n",
    "    #    data = dataset_x, dataset_y\n",
    "    #else:\n",
    "    #    data = prepare_data('relaxed', 'focused', blacklisted_channels={'T3', 'T4'})\n",
    "    data = prepare_data('relaxed', 'focused', blacklisted_channels={'T3', 'T4'})\n",
    "    print_dataset_info(data)\n",
    "    #train_regression_mindfulness(data)\n",
    "    train_svm_mindfulness(data)\n",
    "    #train_knn_mindfulness(data)\n",
    "    #train_random_forest_mindfulness(data)\n",
    "    #train_mlp_mindfulness(data)\n",
    "    #train_stacking_classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484a6492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:1st Class: 0 2nd Class: 0\n",
      "INFO:root:1st Class Dataset Info:\n",
      "INFO:root:Mean:\n",
      "/home/bimanjaya/.conda/envs/selebor/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  `ndarray`, however any non-default value will be.  If the\n",
      "/home/bimanjaya/.conda/envs/selebor/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return False\n",
      "INFO:root:nan\n",
      "INFO:root:2nd Class Dataset Info:\n",
      "INFO:root:Mean:\n",
      "INFO:root:nan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_regression_mindfulness' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-c35c93b5ab3b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relaxed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'focused'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblacklisted_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'T3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'T4'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint_dataset_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_regression_mindfulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_svm_mindfulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_knn_mindfulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_regression_mindfulness' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a66548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
